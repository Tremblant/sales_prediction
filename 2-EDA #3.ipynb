{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Linear Regression\n",
    "\n",
    "    Regularised regression models can handle the correlated independent variables well and helps in overcoming overfitting. Ridge penalty shrinks the coefficients of correlated predictors towards each other, while the Lasso tends to pick one of a pair of correlated features and discard the other.\n",
    "    \n",
    "## l2 Regularization or Ridge Regression\n",
    "\n",
    "    To understand Ridge Regression, we need to remind ourselves of what happens during gradient descent, when our model coefficients are trained. During training, our initial weights are updated according to a gradient update rule using a learning rate and a gradient. Ridge regression adds a penalty to the update, and as a result shrinks the size of our weights. This is implemented in scikit-learn as a class called Ridge.\n",
    "    \n",
    "    We will create a new pipeline, this time using Ridge. We will specify our regularization strength by passing in a parameter, alpha. This can be really small, like 0.1, or as large as you would want it to be. The larger the value of alpha, the less variance your model will exhibit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime #to access datetime\n",
    "from pandas import Series, DataFrame #to work on series & dataframe\n",
    "from pathlib import Path #to create path to directories and files\n",
    "\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings #to ignore the warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pbpython.com/notebook-process.html\n",
    "today = datetime.today()\n",
    "train_original = Path.cwd() /'data'/'raw'/'Train_File.csv'\n",
    "test_original = Path.cwd() /'data'/'raw'/'Test_File.csv'\n",
    "summary_file_train = Path.cwd() /'data'/'processed'/f'summary_train{today:%b-%d-%Y}.pkl'\n",
    "summary_file_test = Path.cwd() /'data'/'processed'/f'summary_test{today:%b-%d-%Y}.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading data\n",
    "train = pd.read_pickle(summary_file_train)\n",
    "test = pd.read_pickle(summary_file_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['Item_Identifier', 'Outlet_Identifier'], axis=1)\n",
    "test = test.drop(['Item_Identifier', 'Outlet_Identifier'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Item_Outlet_Sales', axis=1)\n",
    "y = train['Item_Outlet_Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_cv, y_train, y_cv = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.622802706418365\n",
      "Test Score: 0.5725345939909212\n"
     ]
    }
   ],
   "source": [
    "steps = [\n",
    "    ('scalar', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('model', Ridge(alpha=10, fit_intercept=True))\n",
    "]\n",
    "\n",
    "ridge_pipe = Pipeline(steps)\n",
    "ridge_pipe.fit(x_train, y_train)\n",
    "\n",
    "print('Training Score: {}'.format(ridge_pipe.score(x_train, y_train)))\n",
    "print('Test Score: {}'.format(ridge_pipe.score(x_cv, y_cv)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l1 Regularization or Lasso Regression\n",
    "\n",
    "    By creating a polynomial model, we created additional features. The question we need to ask ourselves is which of our features are relevant to our model, and which are not.\n",
    "    \n",
    "    l1 regularization tries to answer this question by driving the values of certain coefficients down to 0. This eliminates the least important features in our model. We will create a pipeline similar to the one above, but using Lasso. You can play around with the value of alpha, which can range from 0.1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.6227984435265985\n",
      "Test Score: 0.5728321745528362\n"
     ]
    }
   ],
   "source": [
    "steps = [\n",
    "    ('scalar', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('model', Lasso(alpha=0.3, fit_intercept=True))\n",
    "]\n",
    "\n",
    "lasso_pipe = Pipeline(steps)\n",
    "lasso_pipe.fit(x_train, y_train)\n",
    "\n",
    "print('Training Score: {}'.format(lasso_pipe.score(x_train, y_train)))\n",
    "print('Test Score: {}'.format(lasso_pipe.score(x_cv, y_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1088.6554394193124\n"
     ]
    }
   ],
   "source": [
    "#predicting on cv\n",
    "pred2_cv = ridge_pipe.predict(x_cv)\n",
    "\n",
    "#calculating rmse\n",
    "mse = mean_squared_error(y_cv, pred2_cv)\n",
    "rmse = math.sqrt(mse)\n",
    "\n",
    "print('RMSE: {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1088.276439043809\n"
     ]
    }
   ],
   "source": [
    "#predicting on cv\n",
    "pred3_cv = lasso_pipe.predict(x_cv)\n",
    "\n",
    "#calculating rmse\n",
    "mse = mean_squared_error(y_cv, pred3_cv)\n",
    "rmse = math.sqrt(mse)\n",
    "\n",
    "print('RMSE: {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
